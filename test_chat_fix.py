#!/usr/bin/env python3
"""
æµ‹è¯•èŠå¤©æ¥å£ä¿®å¤çš„è„šæœ¬
"""
import asyncio
import json
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.config import config
from src.llm import LLM

async def test_llm_initialization():
    """æµ‹è¯•LLMåˆå§‹åŒ–æ˜¯å¦æ­£å¸¸"""
    print("æµ‹è¯•LLMåˆå§‹åŒ–...")
    try:
        # æµ‹è¯•é»˜è®¤é…ç½®
        llm = LLM(config_name="default")
        print(f"âœ“ LLMåˆå§‹åŒ–æˆåŠŸ")
        print(f"  - æ¨¡å‹: {llm.model}")
        print(f"  - APIç±»å‹: {llm.api_type}")
        print(f"  - åŸºç¡€URL: {llm.base_url}")
        return True
    except Exception as e:
        print(f"âœ— LLMåˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_chat_handler():
    """æµ‹è¯•èŠå¤©å¤„ç†å™¨"""
    print("\næµ‹è¯•èŠå¤©å¤„ç†å™¨...")
    try:
        # å¯¼å…¥èŠå¤©å¤„ç†å™¨
        from backend.chat_handler import ModelConfigManager, ChatSession
        
        # æµ‹è¯•æ¨¡å‹é…ç½®ç®¡ç†å™¨
        model_manager = ModelConfigManager()
        available_models = model_manager.get_model_names()
        print(f"âœ“ å¯ç”¨æ¨¡å‹: {available_models}")
        
        # æµ‹è¯•èŠå¤©ä¼šè¯åˆ›å»º
        session_id = "test_session"
        message = "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ¶ˆæ¯"
        session = ChatSession(session_id, message, model_name="default")
        print(f"âœ“ èŠå¤©ä¼šè¯åˆ›å»ºæˆåŠŸ")
        
        return True
    except Exception as e:
        print(f"âœ— èŠå¤©å¤„ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("\næµ‹è¯•é…ç½®åŠ è½½...")
    try:
        # æ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®åŠ è½½
        llm_configs = config.llm
        print(f"âœ“ LLMé…ç½®åŠ è½½æˆåŠŸ")
        print(f"  - é…ç½®é”®: {list(llm_configs.keys())}")
        
        # æ£€æŸ¥é»˜è®¤é…ç½®
        if "default" in llm_configs:
            default_config = llm_configs["default"]
            print(f"âœ“ é»˜è®¤é…ç½®å­˜åœ¨")
            print(f"  - æ¨¡å‹: {default_config.model}")
            print(f"  - APIç±»å‹: {default_config.api_type}")
        else:
            print("âœ— é»˜è®¤é…ç½®ä¸å­˜åœ¨")
            return False
            
        return True
    except Exception as e:
        print(f"âœ— é…ç½®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•èŠå¤©æ¥å£ä¿®å¤...")
    
    tests = [
        ("é…ç½®åŠ è½½", test_config_loading),
        ("LLMåˆå§‹åŒ–", test_llm_initialization),
        ("èŠå¤©å¤„ç†å™¨", test_chat_handler),
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\n{'='*50}")
        print(f"è¿è¡Œæµ‹è¯•: {test_name}")
        print(f"{'='*50}")
        
        try:
            result = await test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âœ— æµ‹è¯• {test_name} å‡ºç°å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
    print(f"\n{'='*50}")
    print("æµ‹è¯•ç»“æœæ‘˜è¦")
    print(f"{'='*50}")
    
    passed = 0
    for test_name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{len(results)} æµ‹è¯•é€šè¿‡")
    
    if passed == len(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¿®å¤æˆåŠŸï¼")
        return True
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥")
        return False

if __name__ == "__main__":
    asyncio.run(main())